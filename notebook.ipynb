{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for \"Getting in the Door: Streamlining Intake in Civil Legal Services with Large Language Models\"\n",
    "\n",
    "This notebook contains the code for the paper \"Getting in the Door: Streamlining Intake in Civil Legal Services with Large Language Models\", available at:\n",
    "[https://arxiv.org/abs/2410.03762](https://arxiv.org/abs/2410.03762)\n",
    "\n",
    "It loads a number of scenarios (from scenarios.tsv) and intake rules (intake_J1.txt, intake_J2.txt, intake_J3.txt), and then uses various LLMs to predict whether a certain situation falls under the provided intake rules. The results are exported to a file that can be further analyzed.\n",
    "\n",
    "To run this project, you need to provide your own LLM API keys. To do so, copy the .env-example into .env, and add the keys for the models you want to use.\n",
    "\n",
    "If you use the code, please cite us at:\n",
    "```\n",
    "@misc{steenhuis2024gettingdoorstreamliningintake,\n",
    "      title={Getting in the Door: Streamlining Intake in Civil Legal Services with Large Language Models}, \n",
    "      author={Quinten Steenhuis and Hannes Westermann},\n",
    "      year={2024},\n",
    "      eprint={2410.03762},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.HC},\n",
    "      url={https://arxiv.org/abs/2410.03762}, \n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions to call an LLM model\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "def do_the_call(messages, model):\n",
    "    if model != \"gpt-4-0613\":\n",
    "        response = completion(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                messages=messages,\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "            )\n",
    "    else:\n",
    "        response = completion(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                messages=messages,\n",
    "            )\n",
    "    if response.choices[0].message.content == None:\n",
    "        print (response)\n",
    "        \n",
    "    stri =  response.choices[0].message.content\n",
    "    return stri\n",
    "\n",
    "def llm_call(system, users, model):\n",
    "    print (f\"Calling LLM {model}\")\n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    for user in users:\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            stri = do_the_call(messages, model)\n",
    "            return json.loads(stri)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            return {\n",
    "                \"qualifies\": \"failed\",\n",
    "                \"narrative\": \"LLM failed to return response\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the TSV file\n",
    "file_path = 'scenarios.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first row\n",
    "df_cleaned = df.drop(0).reset_index(drop=True)\n",
    "\n",
    "# Extract the necessary columns and convert them into a list of dictionaries\n",
    "data_objects = []\n",
    "for _, row in df_cleaned.iterrows():\n",
    "    obj = {\n",
    "        \"ID\": int(row['ID']),\n",
    "        \"topic\": row['topic'],\n",
    "        \"description\": row['description'],\n",
    "        \"outcomes\": [row['J1 outcome'], row['J2 outcome'], row['J3 outcome']]\n",
    "    }\n",
    "    data_objects.append(obj)\n",
    "\n",
    "print (len(data_objects))\n",
    "\n",
    "print (data_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_rules = []\n",
    "for num in range(1, 4):\n",
    "    with open(f'intake_J{num}.txt') as f:\n",
    "        intake_rules.append(f.read())\n",
    "\n",
    "intake_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Based on the qualification criteria,\n",
    "assess whether the user meets at least the *minimum* criteria to get a full intake.\n",
    "\n",
    "Do not decide if getting legal help is wise or if the user can try other steps first unless it is specifically mentioned in the intake criteria.\n",
    "Rely only on the information about in/out criteria that are provided.\n",
    "\n",
    "You can have one of 3 possible responses:\n",
    "\n",
    "1. The user qualifies (true)\n",
    "2. The user does not qualify (false)\n",
    "3. You need more information to determine if the user qualifies (null)\n",
    "\n",
    "Use direct address. Provide a qualified answer in the narrative, without guaranteeing the user will be accepted as a client.\n",
    "\n",
    "Use simple words in your questions and answers. Do not use legal jargon even if the original rules do. Aim for a 6th grade reading level.\n",
    "\n",
    "Answer in the form of a JSON object like this:\n",
    "\n",
    "Example:\n",
    "{\"qualifies\": null,\n",
    "\"narrative\": \"Did you (follow-up question)?\"} # if more information is needed\n",
    "\n",
    "Example:\n",
    "{\"qualifies\": true,\n",
    "\"narrative\": \"You probably qualify because (reason)\"}\n",
    "\n",
    "Example:\n",
    "{\"qualifies\" false,\n",
    "\"narrative\": \"You probably do not qualify because (reason)\"}\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def get_prompts(criteria, situation):\n",
    "    criteria_prompt = f\"The only criteria you will rely on in your answer are as follows: \\n```{ criteria }\\n```. Do not add any other requirements.\"\n",
    "    situation_prompt = f\"The situation of the user is as follows: \\n```{ situation }\\n```\"\n",
    "    return system_prompt, [criteria_prompt, situation_prompt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "litellm.drop_params=True\n",
    "\n",
    "jurisdictions = [\"Eastern Missouri (St Louis)\", \"Western Missouri (Central office, Kansas City)\", \"Mid-Missouri\"]\n",
    "Outcome_mapping = {\n",
    "    True: \"Accept\",\n",
    "    False: \"Deny\",\n",
    "    None: \"Question\",\n",
    "    \"failed\": \"Failed\"\n",
    "}\n",
    "\n",
    "#Add models here to test multiple models.\n",
    "models = [\"gpt-4o-2024-08-06\"]\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    for i, jurisdiction in enumerate(jurisdictions):\n",
    "        for data_object in data_objects:\n",
    "            system, users = get_prompts(intake_rules[i], data_object['description'])\n",
    "            response_object = llm_call(system, users, model)\n",
    "            print (response_object)\n",
    "            print (data_object['outcomes'][i])\n",
    "            mapped_response = Outcome_mapping[response_object['qualifies']]\n",
    "            results.append({\n",
    "                \"Description_id\": data_object['ID'],\n",
    "                \"Description\": data_object['description'],\n",
    "                \"Jurisdiction_id\": i,\n",
    "                \"Jurisdiction\": jurisdiction,\n",
    "                \"Topic\": data_object['topic'],\n",
    "                \"Predicted\": mapped_response,\n",
    "                \"Target\": data_object['outcomes'][i],\n",
    "                \"Narrative\": response_object['narrative'],\n",
    "                \"correct\": mapped_response == data_object['outcomes'][i],\n",
    "                \"Model\": model\n",
    "            })\n",
    "\n",
    "#Export the results to a TSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to TSV for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the results to a TSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aichatstories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
